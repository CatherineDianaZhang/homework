# 人工智能与伦理

现在关于人工智能的话题越来越多。早期的人工智能只是基于预先设定的程序完成一些需要进行大量计算的具体任务，如下象棋、进行图片分类、回答简单问题等。当前借助深度学习等最新技术，一些人工智能设备实现了全自动化，能够自主推理解决现实生活中的问题，如语言翻译、自动交易、人脸识别、无人驾驶等。人工智能技术的开发和应用将深刻改变人类的生活，不可避免会冲击现有的伦理与社会秩序，带来一些伦理与社会问题。

---

在陈立鹏发表的《[人工智能引发的科学技术伦理问题](https://wenku.baidu.com/view/beac3d073c1ec5da50e270f2.html)》一文中，他将人工智能发展所带来的伦理问题分为两类，即

(1)人工智能的情感问题

(2)人工智能的责任问题

关于人工智能的情感问题，在[豆瓣小组](
https://www.douban.com/group/topic/44706237/)中，又将这一问题分成了几个具体的伦理问题：

第一个伦理问题：如果在没有痛觉神经作为惩罚手段的情况下，机器人无法进行有效控制，我们是否应该为了更好的驱使它们给它们加入痛觉神经元？ 
第二个伦理问题：我们对于以利用AI痛觉神经元进行虐待机器人的行为是否该表示谴责？ 

第三个伦理问题：对于一部分无主的（主人死亡或者遗弃之类的）机器人，我们是应该进行销毁还是回收重新分配，或者给予自由身份？ 

第四个伦理问题：我们是否应当支持某些人呼吁的给予机器人与人类同等身份的观点，比如自由权，选举权？（请注意一点，机器人天然拥有比人类更大的竞争优势）

---

在经济日报发表的《[人工智能三连问](https://baijiahao.baidu.com/s?id=1605573356406135313&wfr=spider&for=pc)》中对人工智能带来的伦理挑战有一下观点：

>随着人工智能的不断发展，我们对人的理解越来越物化和去意义化，人和机器的边界越来越模糊，我们需要思考这种边界模糊的后果。我们该如何对待机器和自身的关系？人和机器应该整合吗？如果人对快乐和痛苦的感受可以通过其他的物理和化学手段来满足，那么人还有参与社会互动的需要和动力吗？

>人工智能还带来了新的社会权力结构问题。借助人工智能，企业可以赋予每个用户大量的数据标签，基于这些标签了解人的偏好和行为，甚至超过用户对自己的了解，这是巨大的权利不对称。

>此外，人工智能可能会造成偏见强化。在社交媒体中，人工智能将观点相近的人相互推荐，新闻推送也常常存在路径依赖。当人们的信息来源越来越依赖于智能机器，偏见会在这种同化和路径依赖中被强化。

>人工智能使社会的信息和知识加工处理能力被极大地放大，信息和知识的冗余反而使人陷入选择困境。如果人参与社会互动的次数和范围缩小，而人工智能越来越多地介入到知识的生产中，知识与人的需求之间的关系变得越来越间接，甚至会反过来支配了人的需求。

>尽管人工智能也会推动进一步的专业化分工和创造新的工作机会，但并非所有人都有能力迈过技术性和社会性壁垒。尤瓦尔·赫拉利在《未来简史》中警告，未来社会可能会出现“无用阶级”，这种担心并非全无道理。

>人工智能也对传统的法律法规和社会规范提出了挑战。譬如，无人驾驶汽车一旦出现事故，我们究竟因该归因于开发产品的企业，产品拥有者还是人工智能产品本身？

---

在发表于《自然》杂志上的一篇评论文章中，计算机领域的研究领军者们分享了他们对该领域的思考。

应对人工智能武器表明立场——Stuart Russel（美国加州大学伯克利分校计算机科学教授）

>LAWS通过允许机器选择杀死谁，而可能侵犯人类尊严的基本原则。在我看来,最应当关注的是这个技术发展轨迹的可能终点。我认为超过人类控制的系统将建成这一点是不可避免的。当然这些武器系统能涉及的范围和携带大型武器的能力会受到物理定律的限制。然而它们的灵活性和杀伤力将让人们毫无防御之力。这样的未来不会是人们想要的。
 
>因此，人工智能和机器人领域的科学家及其所在的专业组织应当表明立场，正如物理学家当年对于核武器，抑或是生物学家对于在战争中使用病原体表明立场一样。应当召开学术会议进行讨论，并让伦理委员会参与进来。什么都不做就等于是表示支持继续发展和使用。

公平分配人工智能福利——Russ Altman（美国斯坦福大学生物工程、基因学、医学和计算科学教授）

>我有两个担忧。首先，人工智能技术将加剧目前的医疗卫生不平衡状况，除非能发现一种方式，让所有病人受益。例如，在美国，失业人员会经历各种水平的护理。只有特殊群体或能支付得起的人，才能获益于先进的诊断技术，这是不公平、不合理的。

>第二，我担忧临床医生理解和解释高性能人工智能系统所得出的结果的能力。大多数卫生保健提供者不会接受一个来自决策支持系统的没有明确描述如何和为何能出现效果的复杂治疗建议。因此，研究人员需要让医生、护士、患者和其他人明白他们应该如何使用人工智能，并且如何公平地使用。

---

对于人工智能的伦理与价值观，有阿西洛马人工智能原则：

2017年1月初举行的“Beneficial AI”会议为基础上建立起来的“阿西洛马人工智能原则”，名称来自此次会议的地点–美国加州的阿西洛马（Asilomar）市，旨在确保AI为人类利益服务。本次会议参加者是业界最富盛名的领袖，如DeepMind首席执行官Demis Hassabis和Facebook AI负责人Yann LeCun等。全球2000多人，包括844名人工智能和机器人领域的专家已联合签署该原则，呼吁全世界的人工智能领域在发展AI的同时严格遵守这些原则，共同保障人类未来的利益和安全。

这一系列原则目前共23项，分为三大类，分别为：科研问题（Research Issues）、伦理和价值（Ethics and values）、更长期的问题（Longer-term Issues）。

其中，伦理与价值类别中有：

安全性：人工智能系统在它们整个运行过程中应该是安全和可靠的，而且其可应用性的和可行性应当接受验证。

故障透明性：如果一个人工智能系统造成了损害，那么造成损害的原因要能被确定。

司法透明性：任何自动系统参与的司法判决都应提供令人满意的司法解释以被相关领域的专家接受。

责任：高级人工智能系统的设计者和建造者，是人工智能使用、误用和行为所产生的道德影响的参与者，有责任和机会去塑造那些道德影响。

价值归属：高度自主的人工智能系统的设计，应该确保它们的目标和行为在整个运行中与人类的价值观相一致。

人类价值观：人工智能系统应该被设计和操作，以使其和人类尊严、权力、自由和文化多样性的理想相一致。

个人隐私：在给予人工智能系统以分析和使用数据的能力时，人们应该拥有权力去访问、管理和控制他们产生的数据。

自由和隐私：人工智能在个人数据上的应用不能充许无理由地剥夺人们真实的或人们能感受到的自由。

分享利益：人工智能科技应该惠及和服务尽可能多的人。

共同繁荣：由人工智能创造的经济繁荣应该被广泛地分享，惠及全人类。

人类控制：人类应该来选择如何和决定是否让人工智能系统去完成人类选择的目标。

非颠覆：高级人工智能被授予的权力应该尊重和改进健康的社会所依赖的社会和公民秩序，而不是颠覆。

人工智能军备竞赛：致命的自动化武器的装备竞赛应该被避免。

---

## 总结

计算机技术的前沿一直由人工智能所占据，在我们生活的现实生活中有很多人工智能的成果已经进入人们的日常生活中。将来，人工智能技术的发展将会给人们的生活、工作和教育等带来更大的影响。

人工智能背后，牵扯的是社会学、伦理学、心理学等学科的问题。我们只有及时对人工智能技术的伦理问题进行反思、总结并采取相应的应对策略，才能使人工智能技术趋利避害，为人类牟取更多福利。